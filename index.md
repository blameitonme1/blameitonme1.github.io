Hi there! My name is Yibo Zhong and this is a website for a quick introduction of myself :) Please contact me via zhongyibo@stu.scu.edu.cn or yibozhong657@gmail.com

![[Me]](https://raw.githubusercontent.com/blameitonme1/blameitonme1.github.io/main/me.png)  

# About me

I'm now an undergraduate at Sichuan University majoring in Computer Science and now I'm in my second year. I currently have had one researching experience. I mainly focus on computer vision and model compression. I'm looking for intern position, please contact me if you're interested!

# Research I have done

Currently I have one paper in [My Paper](https://arxiv.org/abs/2404.08894). It's about the very first project I have done about Parameter Efficient Transfer Learning (or Fine-tuning if you like) in computer vision. More specifically, I took inspiration from many exisitng PETL methods aiming to reduce various redundancies in vision transformer (ViT) to get their siginificantly reduced parameters. By trying to find whether there's any kind of redundancy that is not fully exploited to improve the performance, I found that the **multi-head design** is not fully studied in PETL methods, and studies have found that it actually has redundancy becuase of the multi-heads design, with many heads showing similar patterns. Therefore, I come up with a method dealing with this and you can find it in my paper mentioned above.

# Research Interests

My future research interests are improving machine learning models in various aspects. I think this can be achieved by:

- Learning **foundation models**: nowadays transformer is the most popular foundation model. But many studies are trying to improve it with linear attention or RNN-like archiitectures like rwkv or mamba, which is sort of popular right now. I find this area promising and exciting because it has the potential to impact the most areas where foundation modles are beging used.
- Efficient **adaptation** of models: namely PETL. Having an efficient foundation model is not enough, you may want to adapt it to the field where you are focusing on like NLP, CV or multi-media. This often comes down to adapting pre-trained models to various downstream tasks. Say you are dealing with medical images, you can't directly use a ViT pretrained on ImageNet. When you adapt it to the medical images, let's say there are many kinds of medical images like tumor, teeth, etc, you may want to use the backbone model repeatedly. That means you cant directly fine-tune the model, which is **costly** (lots of parameters) and performance is not that great. So PETL methods are **really** useful in such case.

I'm also **generally interested in the application side**, hope to find an opportunity about it :).

# Personal

#### Big fan of sports

I like badminton and self-weight workout. I used to go to the gym but now I think simply self-weight is enough. You just spent 30 mintues and that's it. No more comuting and money need when you went to the gym. When I think it's time to do some cardio, I run or play badminton, which is super fun and I'm quite good at it.

If you happen to be in my city and good at badminton, we can find time to play badminton together!
